{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01d14c25100541499c24f960a14ffe75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0324fd8db01a4b9ea8911a885dc5fa6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0821a73281a044488d342ce77811aa8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1967b434eb7e4b7384742c133be88a4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19ae477467914cfd84e9910f570c5918":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c05b2dd78b945bfb79fbf1d589e5a30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deec6ac613f94bcd8613543fd70538b9","IPY_MODEL_d901403b2d0c4493b025ff493189535f","IPY_MODEL_8ba953bb631b4dc7bb6c04ebc0322c7f"],"layout":"IPY_MODEL_6a834fd89b524916ad7835acaffbc15a"}},"230e0af0dd44484da484acdb827bafda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25f70d4326f24de6a5be79833e8dae6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"271968f8d1ab424bb22e019b3918f572":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2edc39ab7cac4384a1c4d9343bba711c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b20fbd056bf4a0f9e461f6560c86ba9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42e429997b7748e083fc11ffa6ceeb06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c05a1251826b4426aef3beb231f6bc20","IPY_MODEL_495bf6bc28eb4557a261fa45241531e8","IPY_MODEL_9ab123fe278c4458a36af26e3b4d4c12"],"layout":"IPY_MODEL_2edc39ab7cac4384a1c4d9343bba711c"}},"495bf6bc28eb4557a261fa45241531e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_748bbaba62c0488cb0d4b769b363e4ff","max":990345061,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55144e5c2b9f45e7a0c8e52ede5ae2e1","value":990345061}},"4f6846868bfc4e009d4182d2b0cbff9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f7f5f316583498ba0182adbf2eb3ecf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"532c03e00ff947f88006d24dd61834fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f82cd7f7b8f54ecf8e7c10261426005e","IPY_MODEL_b33377bd5b1e4863b3cb524d3569393f","IPY_MODEL_f17e2fefb943492faf0bb8ab29f28c29"],"layout":"IPY_MODEL_813995ad728944b6b11391d68665c2d9"}},"55144e5c2b9f45e7a0c8e52ede5ae2e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"557c4125b05d4a2bb5aba9c3a35cdc76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_01d14c25100541499c24f960a14ffe75","style":"IPY_MODEL_8b5c4f27115d422caf9488504dccf6ff","value":true}},"60bbd41a9e594913be07404d15d14e7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ba59663d924c3a82ac769a3539d063":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a834fd89b524916ad7835acaffbc15a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d02d8d162b542c9b0ffc79cf2280f52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748bbaba62c0488cb0d4b769b363e4ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7558a44baf9a41bab5fbf1a455128b96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c5dee7e66f74798b82c6f4dbdef1de7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d7c62ffc8734a3fb5b571cd24b47855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_d3bb89745ef04aca8c8d439549e5e609","style":"IPY_MODEL_ec13ab385c52454facea3a3b373d588c","tooltip":""}},"813995ad728944b6b11391d68665c2d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8900e91e67e546769eccfbc380b4dc5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89eed8f1703442058ed94454b349ef7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_b84ea5c4fbd240d299450d3bace12e69","IPY_MODEL_c526b0d3a74d4e7db512925a35f10c8e","IPY_MODEL_557c4125b05d4a2bb5aba9c3a35cdc76","IPY_MODEL_7d7c62ffc8734a3fb5b571cd24b47855","IPY_MODEL_c4a8e0e397d245ec8c33699bbc0fc134"],"layout":"IPY_MODEL_4f7f5f316583498ba0182adbf2eb3ecf"}},"8b5c4f27115d422caf9488504dccf6ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ba953bb631b4dc7bb6c04ebc0322c7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_271968f8d1ab424bb22e019b3918f572","placeholder":"​","style":"IPY_MODEL_7558a44baf9a41bab5fbf1a455128b96","value":" 147/147 [00:00&lt;00:00, 3.79kB/s]"}},"9ab123fe278c4458a36af26e3b4d4c12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d02d8d162b542c9b0ffc79cf2280f52","placeholder":"​","style":"IPY_MODEL_bb2bc7d5a2b6462893179e8fd3e1e1df","value":" 990M/990M [00:09&lt;00:00, 112MB/s]"}},"9de81bae9c9c45f5bf05a971c9b95019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b079c5f8fa495ca6f1aea0cf26e401":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae1812b154064d768ae0f1cb88c38ac5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b33377bd5b1e4863b3cb524d3569393f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25f70d4326f24de6a5be79833e8dae6b","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8900e91e67e546769eccfbc380b4dc5e","value":200}},"b84ea5c4fbd240d299450d3bace12e69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de2c446320864857b7f0f5b73c681552","placeholder":"​","style":"IPY_MODEL_230e0af0dd44484da484acdb827bafda","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"bb2bc7d5a2b6462893179e8fd3e1e1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bde9c0b1b7d6473282763191f281b86d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c05a1251826b4426aef3beb231f6bc20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7b079c5f8fa495ca6f1aea0cf26e401","placeholder":"​","style":"IPY_MODEL_0324fd8db01a4b9ea8911a885dc5fa6b","value":"model.safetensors: 100%"}},"c4a8e0e397d245ec8c33699bbc0fc134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ae477467914cfd84e9910f570c5918","placeholder":"​","style":"IPY_MODEL_4f6846868bfc4e009d4182d2b0cbff9f","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"c526b0d3a74d4e7db512925a35f10c8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_9de81bae9c9c45f5bf05a971c9b95019","placeholder":"​","style":"IPY_MODEL_67ba59663d924c3a82ac769a3539d063","value":""}},"d3bb89745ef04aca8c8d439549e5e609":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d901403b2d0c4493b025ff493189535f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0821a73281a044488d342ce77811aa8a","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae1812b154064d768ae0f1cb88c38ac5","value":147}},"de236f6e8c5644589a193429d061f226":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de2c446320864857b7f0f5b73c681552":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deec6ac613f94bcd8613543fd70538b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c5dee7e66f74798b82c6f4dbdef1de7","placeholder":"​","style":"IPY_MODEL_1967b434eb7e4b7384742c133be88a4a","value":"generation_config.json: 100%"}},"ec13ab385c52454facea3a3b373d588c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f17e2fefb943492faf0bb8ab29f28c29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60bbd41a9e594913be07404d15d14e7b","placeholder":"​","style":"IPY_MODEL_bde9c0b1b7d6473282763191f281b86d","value":" 200/200 [00:00&lt;00:00, 660.64 examples/s]"}},"f82cd7f7b8f54ecf8e7c10261426005e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de236f6e8c5644589a193429d061f226","placeholder":"​","style":"IPY_MODEL_3b20fbd056bf4a0f9e461f6560c86ba9","value":"Map: 100%"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8072103,"sourceType":"datasetVersion","datasetId":4763101}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning ","metadata":{"id":"_6IgAkg96Wea"}},{"cell_type":"code","source":"# import locale\n# locale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"id":"0sp0x3V-JV_0","execution":{"iopub.status.busy":"2024-04-09T10:14:50.819188Z","iopub.execute_input":"2024-04-09T10:14:50.819925Z","iopub.status.idle":"2024-04-09T10:14:50.824071Z","shell.execute_reply.started":"2024-04-09T10:14:50.819892Z","shell.execute_reply":"2024-04-09T10:14:50.823089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U datasets scipy","metadata":{"id":"q7EV1ZcP6b3F","outputId":"cf635bb9-da64-4385-b58e-32e6e9f8ddfa","execution":{"iopub.status.busy":"2024-05-11T08:38:57.630777Z","iopub.execute_input":"2024-05-11T08:38:57.631092Z","iopub.status.idle":"2024-05-11T08:41:26.482969Z","shell.execute_reply.started":"2024-05-11T08:38:57.631064Z","shell.execute_reply":"2024-05-11T08:41:26.482033Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n#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\nnotebook_login()","metadata":{"id":"tny1d6XW_YRW","outputId":"132f313d-822a-4b6f-c873-11f3f4ccfedf","execution":{"iopub.status.busy":"2024-05-11T10:51:44.182726Z","iopub.execute_input":"2024-05-11T10:51:44.183611Z","iopub.status.idle":"2024-05-11T10:51:44.207706Z","shell.execute_reply.started":"2024-05-11T10:51:44.183574Z","shell.execute_reply":"2024-05-11T10:51:44.206691Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41d8c29a27754c039d61a8998e5995fe"}},"metadata":{}}]},{"cell_type":"code","source":"from accelerate import FullyShardedDataParallelPlugin, Accelerator\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\nfsdp_plugin = FullyShardedDataParallelPlugin(\n    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n)\n\naccelerator = Accelerator(fsdp_plugin=fsdp_plugin)","metadata":{"id":"ISR2nGve6nG9","execution":{"iopub.status.busy":"2024-05-11T10:51:44.576604Z","iopub.execute_input":"2024-05-11T10:51:44.577016Z","iopub.status.idle":"2024-05-11T10:51:44.590098Z","shell.execute_reply.started":"2024-05-11T10:51:44.576980Z","shell.execute_reply":"2024-05-11T10:51:44.589225Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"AgamP/FAQ\",split=\"train\")","metadata":{"id":"sw7BIlGS-bVK","outputId":"5c569044-4083-421d-814c-4a5f0ed729fc","execution":{"iopub.status.busy":"2024-05-11T10:51:45.291407Z","iopub.execute_input":"2024-05-11T10:51:45.292263Z","iopub.status.idle":"2024-05-11T10:51:46.320778Z","shell.execute_reply.started":"2024-05-11T10:51:45.292225Z","shell.execute_reply":"2024-05-11T10:51:46.319977Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:51:46.322214Z","iopub.execute_input":"2024-05-11T10:51:46.322509Z","iopub.status.idle":"2024-05-11T10:51:46.328618Z","shell.execute_reply.started":"2024-05-11T10:51:46.322482Z","shell.execute_reply":"2024-05-11T10:51:46.327461Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n    num_rows: 26872\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset= dataset.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:51:46.662197Z","iopub.execute_input":"2024-05-11T10:51:46.662991Z","iopub.status.idle":"2024-05-11T10:51:46.689598Z","shell.execute_reply.started":"2024-05-11T10:51:46.662952Z","shell.execute_reply":"2024-05-11T10:51:46.688710Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:51:47.806696Z","iopub.execute_input":"2024-05-11T10:51:47.807070Z","iopub.status.idle":"2024-05-11T10:51:47.813212Z","shell.execute_reply.started":"2024-05-11T10:51:47.807040Z","shell.execute_reply":"2024-05-11T10:51:47.812387Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n        num_rows: 21497\n    })\n    test: Dataset({\n        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n        num_rows: 5375\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,AutoModelForSeq2SeqLM\n\nbase_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)","metadata":{"id":"JQcYa3tk_DU6","outputId":"22e97179-ae3d-4d34-a6eb-242b58bf29c7","execution":{"iopub.status.busy":"2024-05-11T10:51:49.000428Z","iopub.execute_input":"2024-05-11T10:51:49.000793Z","iopub.status.idle":"2024-05-11T10:52:05.269609Z","shell.execute_reply.started":"2024-05-11T10:51:49.000763Z","shell.execute_reply":"2024-05-11T10:52:05.268796Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d76676753c74d16b71f4f5f4325956c"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    model_max_length=512,\n    padding_side=\"left\",\n    add_eos_token=True)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"0xSZxSs6_lSZ","execution":{"iopub.status.busy":"2024-05-11T10:52:05.271657Z","iopub.execute_input":"2024-05-11T10:52:05.272058Z","iopub.status.idle":"2024-05-11T10:52:05.409682Z","shell.execute_reply.started":"2024-05-11T10:52:05.272022Z","shell.execute_reply":"2024-05-11T10:52:05.408790Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:52:05.410937Z","iopub.execute_input":"2024-05-11T10:52:05.411222Z","iopub.status.idle":"2024-05-11T10:52:05.429337Z","shell.execute_reply.started":"2024-05-11T10:52:05.411198Z","shell.execute_reply":"2024-05-11T10:52:05.428257Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:52:05.431088Z","iopub.execute_input":"2024-05-11T10:52:05.431369Z","iopub.status.idle":"2024-05-11T10:52:05.437945Z","shell.execute_reply.started":"2024-05-11T10:52:05.431344Z","shell.execute_reply":"2024-05-11T10:52:05.436930Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"{'flags': 'BL',\n 'instruction': 'help to report a registration problem',\n 'category': 'ACCOUNT',\n 'intent': 'registration_problems',\n 'response': 'Thank you for seeking assistance in reporting a registration problem. We understand that encountering any issues during the registration process can be frustrating. We are here to help you resolve this matter swiftly and efficiently. To better assist you, could you please provide more details about the problem you are facing? This information will enable us to identify the exact issue and provide you with the most suitable solution. Your feedback is highly valuable, and we appreciate your active participation in resolving this matter together.'}"},"metadata":{}}]},{"cell_type":"code","source":"for row in dataset['train']:\n    #print(row)\n    print(row['instruction'])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-11T10:53:15.847498Z","iopub.execute_input":"2024-05-11T10:53:15.848142Z","iopub.status.idle":"2024-05-11T10:53:15.853611Z","shell.execute_reply.started":"2024-05-11T10:53:15.848104Z","shell.execute_reply":"2024-05-11T10:53:15.852569Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"help to report a registration problem\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Preprocess data","metadata":{}},{"cell_type":"markdown","source":"**PROBLEM STEP**","metadata":{}},{"cell_type":"code","source":"def tokenize(prompt):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:00:19.598660Z","iopub.execute_input":"2024-05-11T11:00:19.599077Z","iopub.status.idle":"2024-05-11T11:00:19.604774Z","shell.execute_reply.started":"2024-05-11T11:00:19.599044Z","shell.execute_reply":"2024-05-11T11:00:19.603717Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def generate_and_tokenize_prompt(df):\n    full_prompt = f\"\"\"\n    \n    You are a customer support assistant. Customers will ask questions regarding the application in various steps and ways.\n    The questions can be like {df['instruction']} and your response should be relevant just like {df['response']}\n  \n  Please formulate a thoughtful follow-up solution to address the customer doubt and try to resolve them.\n  Outuput:\n  \"\"\"\n    df['prompt'] = full_prompt\n    return tokenizer(full_prompt)\n\n\ntokenized_data_train=dataset['train'].map(generate_and_tokenize_prompt)\ntokenized_data_test=dataset['test'].map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:00:20.788370Z","iopub.execute_input":"2024-05-11T11:00:20.789172Z","iopub.status.idle":"2024-05-11T11:00:50.334249Z","shell.execute_reply.started":"2024-05-11T11:00:20.789139Z","shell.execute_reply":"2024-05-11T11:00:50.333203Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21497 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bfeb9f587a744d1ba44f73c1b386964"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95270e05c04d4daca471e834b76eaca4"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n\ndef preprocess (data):\n    #for i in data\n    #print(data[0].keys)\n    #data=data[\"train\"]\n    inputs=[f\"Question: {row['instruction']}\\n Category: {row['category']}\\n Intent: {row['intent']}\\n Assistant: \" for row in data]\n    targets=[row[\"response\"] for row in data]\n    model_inputs=tokenizer(inputs,max_length=512,truncation=True,padding=\"max_length\",return_tensors=\"pt\")\n    lebels=tokenizer(tragets,max_length=512,truncation=True,padding=\"max_length\",return_tensors=\"pt\")[\"input_ids\"]\n    \n    model_inputs[\"labels\"]=labels \n    \n    return model_inputs\n    \n    \ntokenized_data_train=dataset['train'].map(preprocess)\ntokenized_data_test=dataset['test'].map(preporcess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * (trainable_params / all_param)}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:01:05.261787Z","iopub.execute_input":"2024-05-11T11:01:05.262130Z","iopub.status.idle":"2024-05-11T11:01:05.267858Z","shell.execute_reply.started":"2024-05-11T11:01:05.262103Z","shell.execute_reply":"2024-05-11T11:01:05.266826Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig,get_peft_model\n\nlora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    #target_modules=[\n        #\"query_key_value\",\n        #\"dense\",\n        #\"dense_h_to_4h\",\n        #\"dense_4h_to_h\",\n    #]\n)\nmodel = get_peft_model(model,peft_config)\nprint_trainable_parameters(model)\n\n# Apply the accelerator. You can comment this out to remove the accelerator.\n#model = accelerator.prepare_model(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T11:01:05.665138Z","iopub.execute_input":"2024-05-11T11:01:05.665770Z","iopub.status.idle":"2024-05-11T11:01:06.186016Z","shell.execute_reply.started":"2024-05-11T11:01:05.665718Z","shell.execute_reply":"2024-05-11T11:01:06.184930Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"trainable params: 27262976 || all params: 3779334144 || trainable%: 0.7213698223345028\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 2\ngradient_accumulation_steps = 8\noptim = \"paged_adamw_8bit\"\nsave_steps = 2\nlogging_steps = 2\nlearning_rate = 2e-5\nmax_grad_norm = 0.3\nmax_steps = 10\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    gradient_checkpointing=True,\n    do_eval=True,\n    report_to=\"wandb\"\n    #offload_state_dict=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:47.337407Z","iopub.execute_input":"2024-05-11T12:05:47.338246Z","iopub.status.idle":"2024-05-11T12:05:47.374616Z","shell.execute_reply.started":"2024-05-11T12:05:47.338213Z","shell.execute_reply":"2024-05-11T12:05:47.373878Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nfrom trl import SFTTrainer\nfrom transformers import DataCollatorForLanguageModeling\n\nproject = \"customer_chatbot-finetune\"\nbase_model_name = \"mistral\"\nrun_name = base_model_name + \"-\" + project\noutput_dir = \"./\" + run_name\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_data_train,\n    eval_dataset=tokenized_data_train,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    args=training_arguments,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:48.165415Z","iopub.execute_input":"2024-05-11T12:05:48.165832Z","iopub.status.idle":"2024-05-11T12:05:48.368485Z","shell.execute_reply.started":"2024-05-11T12:05:48.165797Z","shell.execute_reply":"2024-05-11T12:05:48.367489Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:50.047964Z","iopub.execute_input":"2024-05-11T12:05:50.048789Z","iopub.status.idle":"2024-05-11T12:05:50.060698Z","shell.execute_reply.started":"2024-05-11T12:05:50.048730Z","shell.execute_reply":"2024-05-11T12:05:50.059563Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"import wandb","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:51.142604Z","iopub.execute_input":"2024-05-11T12:05:51.143329Z","iopub.status.idle":"2024-05-11T12:05:51.148842Z","shell.execute_reply.started":"2024-05-11T12:05:51.143294Z","shell.execute_reply":"2024-05-11T12:05:51.147768Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"#76c5540e566b2cccffeccfd2fcb1386814e667c4","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:52.458940Z","iopub.execute_input":"2024-05-11T12:05:52.459725Z","iopub.status.idle":"2024-05-11T12:05:52.465416Z","shell.execute_reply.started":"2024-05-11T12:05:52.459689Z","shell.execute_reply":"2024-05-11T12:05:52.464337Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:05:53.711862Z","iopub.execute_input":"2024-05-11T12:05:53.712247Z","iopub.status.idle":"2024-05-11T12:33:34.966991Z","shell.execute_reply.started":"2024-05-11T12:05:53.712217Z","shell.execute_reply":"2024-05-11T12:33:34.965197Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 21:19, Epoch 0.01/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2</td>\n      <td>1.434200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.647200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.638500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.695600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:52:39.032845Z","iopub.execute_input":"2024-05-11T12:52:39.033566Z","iopub.status.idle":"2024-05-11T12:52:39.114726Z","shell.execute_reply.started":"2024-05-11T12:52:39.033534Z","shell.execute_reply":"2024-05-11T12:52:39.113614Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:52:41.392342Z","iopub.execute_input":"2024-05-11T12:52:41.393067Z","iopub.status.idle":"2024-05-11T12:52:46.278026Z","shell.execute_reply.started":"2024-05-11T12:52:41.393032Z","shell.execute_reply":"2024-05-11T12:52:46.276841Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715426041.8d69fa7a2964.34.2:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f252a13d5a64431a1a4d3a2356b7fce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 8 LFS files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05fb7d4a90f404aa6190c6c2e44053a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8067dc2700a74a39a047f28e8d18ea35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715425434.8d69fa7a2964.34.0:   0%|          | 0.00/5.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4588d650ee4e519a77550c87287266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715427563.8d69fa7a2964.34.3:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048d1e3cf5024b7d81ced0060511cd32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715425872.8d69fa7a2964.34.1:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5700956c52b4a589843fc32506fbde7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1715428484.8d69fa7a2964.34.4:   0%|          | 0.00/5.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bfb525b6d50467292c77b70e27aeac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236c60ff0b5f4f689585fc38c4416c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2ceab043134c14b9109eda80e2bd2e"}},"metadata":{}},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AgamP/results/commit/69b161277055ffaf8ecb55f6e350a472006776b2', commit_message='End of training', commit_description='', oid='69b161277055ffaf8ecb55f6e350a472006776b2', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(trainer,\"ft-e50\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"ft-e50\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:53:15.810916Z","iopub.execute_input":"2024-05-11T12:53:15.811849Z","iopub.status.idle":"2024-05-11T12:53:16.169486Z","shell.execute_reply.started":"2024-05-11T12:53:15.811813Z","shell.execute_reply":"2024-05-11T12:53:16.168189Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\nmax_seq_length = 512\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset['train'],\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(prompt):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=512,\n        padding=\"max_length\",\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"4M6a-asI_DSf","execution":{"iopub.status.busy":"2024-04-10T18:59:28.308171Z","iopub.execute_input":"2024-04-10T18:59:28.30848Z","iopub.status.idle":"2024-04-10T18:59:28.315722Z","shell.execute_reply.started":"2024-04-10T18:59:28.308454Z","shell.execute_reply":"2024-04-10T18:59:28.314934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import DataCollatorForLanguageModeling\n\ndf1 = pd.read_excel('/kaggle/input/dataset-qa/combined_dataset.xlsx')\ndf2=pd.read_excel('/kaggle/input/dataset-qa/Sde_data.xlsx')\ndf3=pd.read_excel('/kaggle/input/dataset-qa/DS_data.xlsx')\ndf4=pd.read_excel('/kaggle/input/dataset-qa/PMConsult_data.xlsx')\n\ndf1=df1.rename(columns={'Position/Role':'Job_Position'})\n\ndf = pd.concat([df1, df2, df3, df4], ignore_index=True)\ndf.drop(columns=['Column1.6','Column1.7'],inplace=True)\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\ndf.to_csv('data.csv',index=False)","metadata":{"id":"nh_wmYfrBnSH","outputId":"ac39bfb9-ceb9-4ec4-f0c4-ff3e82d5d25b","execution":{"iopub.status.busy":"2024-04-10T18:59:28.317106Z","iopub.execute_input":"2024-04-10T18:59:28.317459Z","iopub.status.idle":"2024-04-10T18:59:41.107818Z","shell.execute_reply.started":"2024-04-10T18:59:28.317431Z","shell.execute_reply":"2024-04-10T18:59:41.106662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df_qa,test_df_qa=train_test_split(df,random_state=42)\ntrain_df_qa.reset_index(drop = True,inplace = True)\ntest_df_qa.reset_index(drop = True,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.109318Z","iopub.execute_input":"2024-04-10T18:59:41.109771Z","iopub.status.idle":"2024-04-10T18:59:41.132689Z","shell.execute_reply.started":"2024-04-10T18:59:41.109733Z","shell.execute_reply":"2024-04-10T18:59:41.131652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_data_collator(df):\n    inputs = tokenizer(\n        [f\"\"\"You are now conducting an interview for the {df[\"Job_Position\"]} role.\n        You have asked the candidate the following question:\n          {df['Question']}\n          for which the candidate has give response, you have to now ask him/her a follow up question based on the given response.\n        \"\"\" for example in zip( df)]\n    )\n    \n    labels =tokenizer([f\"Answer: {answer} </s>\" for df['Answer'] in zip(df)])\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels_ids': labels['label_ids']  \n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.134118Z","iopub.execute_input":"2024-04-10T18:59:41.134553Z","iopub.status.idle":"2024-04-10T18:59:41.142737Z","shell.execute_reply.started":"2024-04-10T18:59:41.134512Z","shell.execute_reply":"2024-04-10T18:59:41.14184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_input_and_labels2(examples):\n    job_positions, interview_phases, questions, answers = zip(*examples)\n    inputs = [f\"\"\"\n  You are now conducting an interview for the {df[\"Job_Position\"]} role.\n  \n  You have asked the candidate the following question:\n  {df['Question']}\n  \n  The candidate has responded as follows:\n  {df['Answer']}\n  \n  Please formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience. \n  Additionally, provide a comprehensive assessment of the accuracy, clarity, comprehensiveness, and relevance of the candidate's response in relation to the desired skills \n  and knowledge for the {df[\"Job Position\"]} role.\n  <eos>\n  \"\"\"\n              \n    for job_pos, interview_phase, question in zip(job_positions, interview_phases, questions)]\n    labels = [f\"answer: {answer} </s>\" for answer in answers]\n    \n    input_ids = tokenizer(inputs, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    label_ids = tokenizer(labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    return {\"input_ids\": input_ids, \"labels\": label_ids}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.143935Z","iopub.execute_input":"2024-04-10T18:59:41.144286Z","iopub.status.idle":"2024-04-10T18:59:41.15446Z","shell.execute_reply.started":"2024-04-10T18:59:41.144253Z","shell.execute_reply":"2024-04-10T18:59:41.153664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine(df):\n    prompt= f\"\"\" You are now conducting an interview for the {df['Job_Position']} role.\n    You have asked the candidate the following question:\n  {df['Question']}\\\n    \n    The candidate has responded as follows:\n  {df['Answer']}\\\n  ask him a follow up question.\n    \"\"\"\n    full = tokenize(prompt)\n    \n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-10T18:59:41.222688Z","iopub.execute_input":"2024-04-10T18:59:41.222964Z","iopub.status.idle":"2024-04-10T18:59:41.227715Z","shell.execute_reply.started":"2024-04-10T18:59:41.22294Z","shell.execute_reply":"2024-04-10T18:59:41.226828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_tokenize_prompt(df):\n    full_prompt = f\"\"\"\n  You are now conducting an interview for the {df['Job Position']}\\ role.\n  \n  You have asked the candidate the following question:\n  {df['Question']}\\\n  \n  The candidate has responded as follows:\n  {df['Answer']}\\\n  \n  Please formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the {df[\"Job Position\"]}\\ role.\n  \"\"\"\n    return tokenize(full_prompt)\n","metadata":{"id":"GeptANLS_DP5","execution":{"iopub.status.busy":"2024-04-10T19:02:01.906302Z","iopub.execute_input":"2024-04-10T19:02:01.906715Z","iopub.status.idle":"2024-04-10T19:02:01.912444Z","shell.execute_reply.started":"2024-04-10T19:02:01.906685Z","shell.execute_reply":"2024-04-10T19:02:01.911317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-10T19:02:04.467979Z","iopub.execute_input":"2024-04-10T19:02:04.468379Z","iopub.status.idle":"2024-04-10T19:02:04.47479Z","shell.execute_reply.started":"2024-04-10T19:02:04.468341Z","shell.execute_reply":"2024-04-10T19:02:04.473726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = dataset['train'].map(generate_and_tokenize_prompt)\ntokenized_val_dataset = dataset['test'].map(generate_and_tokenize_prompt)","metadata":{"id":"GKL-jhTX_DLd","outputId":"1466b976-2163-4876-db56-db3e09b32430","execution":{"iopub.status.busy":"2024-04-10T19:02:04.828532Z","iopub.execute_input":"2024-04-10T19:02:04.828938Z","iopub.status.idle":"2024-04-10T19:02:07.721952Z","shell.execute_reply.started":"2024-04-10T19:02:04.828908Z","shell.execute_reply":"2024-04-10T19:02:07.720929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textwrap import wrap\ndef print_wrapped_text(text, width=80):\n    wrapped_text = wrap(text, width=width)\n    for line in wrapped_text:\n        print(line)","metadata":{"id":"I60hTJ_VEE1M","execution":{"iopub.status.busy":"2024-04-10T19:02:09.661478Z","iopub.execute_input":"2024-04-10T19:02:09.661975Z","iopub.status.idle":"2024-04-10T19:02:09.667935Z","shell.execute_reply.started":"2024-04-10T19:02:09.66194Z","shell.execute_reply":"2024-04-10T19:02:09.666678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"uWcHJs5w_DB-","execution":{"iopub.status.busy":"2024-04-10T19:02:10.326684Z","iopub.execute_input":"2024-04-10T19:02:10.327419Z","iopub.status.idle":"2024-04-10T19:02:10.34329Z","shell.execute_reply.started":"2024-04-10T19:02:10.327382Z","shell.execute_reply":"2024-04-10T19:02:10.342388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * (trainable_params / all_param)}\"\n    )","metadata":{"id":"DOaJXM9vG8Xl","execution":{"iopub.status.busy":"2024-04-10T19:02:11.310154Z","iopub.execute_input":"2024-04-10T19:02:11.310537Z","iopub.status.idle":"2024-04-10T19:02:11.316268Z","shell.execute_reply.started":"2024-04-10T19:02:11.310507Z","shell.execute_reply":"2024-04-10T19:02:11.315279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n\n# Apply the accelerator. You can comment this out to remove the accelerator.\nmodel = accelerator.prepare_model(model)","metadata":{"id":"NDaVafdeG8QK","outputId":"b7db0e5f-649e-42a5-841e-27791c03b400","execution":{"iopub.status.busy":"2024-04-10T19:02:12.330481Z","iopub.execute_input":"2024-04-10T19:02:12.330886Z","iopub.status.idle":"2024-04-10T19:02:12.902562Z","shell.execute_reply.started":"2024-04-10T19:02:12.330855Z","shell.execute_reply":"2024-04-10T19:02:12.901608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"id":"FRX9OwjOG8Ny","execution":{"iopub.status.busy":"2024-04-10T19:02:13.864277Z","iopub.execute_input":"2024-04-10T19:02:13.864687Z","iopub.status.idle":"2024-04-10T19:02:13.869527Z","shell.execute_reply.started":"2024-04-10T19:02:13.864638Z","shell.execute_reply":"2024-04-10T19:02:13.868468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom datetime import datetime\n\nproject = \"interview-finetune\"\nbase_model_name = \"mistral\"\nrun_name = base_model_name + \"-\" + project\noutput_dir = \"./\" + run_name\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=transformers.TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=2,\n        per_device_train_batch_size=2,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=50,\n        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n        logging_steps=5,\n        bf16=False,\n        optim=\"paged_adamw_8bit\",\n        tf32=False,\n        logging_dir=\"./logs\",        # Directory for storing logs\n        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n        save_steps=5,                # Save checkpoints every 50 steps\n        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n        eval_steps=5,                # Evaluate and save checkpoints every 50 steps\n        do_eval=True,                # Perform evaluation at the end of training\n        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n# 962f65e904cc5c119e8c9c2b7749e37073e22bce api key","metadata":{"id":"WhqD_a3GG8LR","outputId":"cd3346eb-2ae1-42d4-98e3-8ad91f6acc26","execution":{"iopub.status.busy":"2024-04-10T19:09:29.043544Z","iopub.execute_input":"2024-04-10T19:09:29.044464Z","iopub.status.idle":"2024-04-10T22:40:55.864939Z","shell.execute_reply.started":"2024-04-10T19:09:29.044429Z","shell.execute_reply":"2024-04-10T22:40:55.863283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n# 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\n# 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 write access\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:42:03.456574Z","iopub.execute_input":"2024-04-10T22:42:03.456971Z","iopub.status.idle":"2024-04-10T22:42:03.48599Z","shell.execute_reply.started":"2024-04-10T22:42:03.456943Z","shell.execute_reply":"2024-04-10T22:42:03.48501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import create_repo\n# create_repo(\"mistral-interview-finetune-v_1.0\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:45:28.252147Z","iopub.execute_input":"2024-04-10T22:45:28.252521Z","iopub.status.idle":"2024-04-10T22:45:28.258864Z","shell.execute_reply.started":"2024-04-10T22:45:28.252494Z","shell.execute_reply":"2024-04-10T22:45:28.257539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T22:54:39.530588Z","iopub.execute_input":"2024-04-10T22:54:39.531415Z","iopub.status.idle":"2024-04-10T22:55:14.182303Z","shell.execute_reply.started":"2024-04-10T22:54:39.531378Z","shell.execute_reply":"2024-04-10T22:55:14.180982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n\n# api=HfApi()\n\n# api.upload_folder(\n#     folder_path=\"/kaggle/working/mistral-interview-finetune/checkpoint-35\",\n#     repo_id=\"mistral-interview-finetune-v_1.0\",\n#     repo_type=\"model\",\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U datasets scipy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbase_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n# AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,  # Mistral, same as before\n    quantization_config=bnb_config,  # Same quantization config as before\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\neval_tokenizer = AutoTokenizer.from_pretrained(\n    base_model_id,\n    add_bos_token=True,\n    trust_remote_code=True,\n)","metadata":{"id":"8iIZqxoFG8G6","execution":{"iopub.status.busy":"2024-04-10T22:58:42.021563Z","iopub.execute_input":"2024-04-10T22:58:42.021956Z","iopub.status.idle":"2024-04-10T22:59:39.004609Z","shell.execute_reply.started":"2024-04-10T22:58:42.021926Z","shell.execute_reply":"2024-04-10T22:59:39.003416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\nconfig = PeftConfig.from_pretrained(\"akshatshaw/mistral-interview-finetune\")\n# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel = PeftModel.from_pretrained(base_model, \"akshatshaw/mistral-interview-finetune\")","metadata":{"id":"3CvTroLYG8E0","execution":{"iopub.status.busy":"2024-04-10T22:59:42.22246Z","iopub.execute_input":"2024-04-10T22:59:42.223384Z","iopub.status.idle":"2024-04-10T22:59:53.140951Z","shell.execute_reply.started":"2024-04-10T22:59:42.223349Z","shell.execute_reply":"2024-04-10T22:59:53.139709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_prompt = \"\"\"\nYou are now conducting an interview for the Customer Service Representative role.\nYou have asked the candidate the following question:\nName any Two Improvements You Made in the Previous Company?\nThe candidate has responded as follows:\nAs a few of my team members were late to work, which impacted the work, I changed the login conditions by introducing timely login incentives. Hence, it improved the login time.‚Äù There were a few people who used to miss the calls and sometimes used break aux and stayed on it for longer. Announcing a reward for the highest number of calls attended improved the work balance effectively\n  \nPlease formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the Customer Service Representative role.\n\"\"\"\n\nmodel_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.eval()\nwith torch.no_grad():\n    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))","metadata":{"id":"JojDNKk8HjoR","outputId":"6ebd5ee7-2af7-4188-91a9-e5a3954337b1","execution":{"iopub.status.busy":"2024-04-10T23:03:35.784441Z","iopub.execute_input":"2024-04-10T23:03:35.784828Z","iopub.status.idle":"2024-04-10T23:03:51.476421Z","shell.execute_reply.started":"2024-04-10T23:03:35.7848Z","shell.execute_reply":"2024-04-10T23:03:51.475355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}